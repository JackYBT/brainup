{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "import io\n",
    "from os import path\n",
    "\n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.signal as signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.stats import skew, kurtosis, ttest_ind, ttest_rel\n",
    "import re\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data slicing for one picture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_slicing(input_directory, destination, eval_mode = None, participant = None):\n",
    "    i = 0\n",
    "    \n",
    "    #print(os.listdir(input_directory))\n",
    "\n",
    "    for phase in os.listdir(input_directory):\n",
    "        if phase.find(\".DS_Store\") != -1: continue\n",
    "\n",
    "        directory = input_directory + phase +\"/\"\n",
    "\n",
    "        for filename in os.listdir(directory):\n",
    "            i += 1\n",
    "            if filename.find('txt') == -1: continue\n",
    "            \n",
    "            if eval_mode == True:\n",
    "                label = filename[filename.find(participant):]\n",
    "            else:\n",
    "                if phase.find(\"test\") != -1 or phase.find(\"high\") != -1:\n",
    "                    label = 'high'\n",
    "                elif phase.find(\"medium\") != -1:\n",
    "                    label = 'medium'\n",
    "                elif phase.find(\"low\") != -1:\n",
    "                    label = 'low'\n",
    "\n",
    "            df = np.loadtxt(directory+filename, dtype = 'str', delimiter = ',')\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for row in df:\n",
    "                for iteration in range(5):\n",
    "                    temp = row[iteration*NUM_ELECTRODES+2:(iteration+1)*NUM_ELECTRODES+2].astype(float)\n",
    "                    temp = temp*1000/24 #Conversion from mV to uV\n",
    "                    result.append(temp)\n",
    "\n",
    "            result = np.array(result)  \n",
    "\n",
    "            #Be careful when you cut time\n",
    "            #result = result[hz*time_to_cut:-hz*time_to_cut]\n",
    "            np.save(destination+label+str(i)+ \".npy\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFT & filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out signals\n",
    "def notch_filter(data):\n",
    "    b0,a0 = signal.butter(2, [47.5, 52.5], 'bandstop', fs=250)  #Removing all the signals with frequency 47.5 - 52.5 from the original signal\n",
    "    data_filt0 = signal.filtfilt(b0, a0, data)\n",
    "\n",
    "    \n",
    "    b1,a1 = signal.butter(2, [97.5, 102.5], 'bandstop', fs=250) \n",
    "    notch_filter_data = signal.filtfilt(b1, a1, data_filt0)\n",
    "    return notch_filter_data\n",
    "\n",
    "#Keeping in signals\n",
    "def bandpass_filter(data):\n",
    "    b,a=signal.butter(4,[0.5,45],'bandpass',fs=250)  #Selecting the signals with frequency 0.5 - 32\n",
    "    notch_filter_data = notch_filter(data)\n",
    "    band_filter_data=signal.filtfilt(b,a,notch_filter_data)\n",
    "    return band_filter_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N: length of array T: 1/sampling rate \n",
    "'''def FFT(data, N, T):\n",
    "    print(\"data.shape\",data.shape)\n",
    "    yf = fft(data)\n",
    "    xf = fftfreq(N, T)[:N//2] #returns array with length N because it returns the entire \n",
    "    \n",
    "    #print(xf.shape, yf.shape)\n",
    "    plt.clf()\n",
    "    #plt.xlim(0,35)\n",
    "    plt.plot(xf, 2.0*N * np.abs(yf[0:N//2]))\n",
    "    plt.show()\n",
    "    \n",
    "    return xf, yf\n",
    "\n",
    "def psd_one(fft_value, freq, band):\n",
    "\n",
    "    cond_band = np.where((freq >= FREQ_BANDS[band][0]) & (freq <= FREQ_BANDS[band[1]]))\n",
    "    psd_band = fft_value[:,cond_band].reshape(fft_value.shape[0], -1)\n",
    "    result = np.sum(psd_band, axis = 1)\n",
    "    \n",
    "    return result'''\n",
    "\n",
    "\n",
    "def psd_c1(array, Fs=hz): # returning the sum delta,theta, alpha, sigma, beta, gamma, R for one channel.\n",
    "    \n",
    "    rows = len(array)\n",
    "    freq = fftfreq(rows, 1/Fs)\n",
    "    \n",
    "    #The actual transformation\n",
    "    fft_value = np.abs(fft(array)[:int(rows/2+0.5)])\n",
    "    fft_norm = fft_value/(np.sum(fft_value)) #Normalizing\n",
    "    \n",
    "    # delta\n",
    "    cond_delta = np.where((freq>=FREQ_BANDS['delta'][0]) &\n",
    "                          (freq<=FREQ_BANDS['delta'][1]))\n",
    "    delta = np.sum(fft_norm[cond_delta])\n",
    "    # theta\n",
    "    cond_theta = np.where((freq>=FREQ_BANDS['theta'][0]) &\n",
    "                          (freq<=FREQ_BANDS['theta'][1]))\n",
    "    theta = np.sum(fft_norm[cond_theta])\n",
    "    # alpha\n",
    "    cond_alpha = np.where((freq>=FREQ_BANDS['alpha'][0]) &\n",
    "                          (freq<=FREQ_BANDS['alpha'][1]))\n",
    "    alpha = np.sum(fft_norm[cond_alpha])\n",
    "    # sigma\n",
    "    cond_sigma = np.where((freq>=FREQ_BANDS['sigma'][0]) &\n",
    "                          (freq<=FREQ_BANDS['sigma'][1]))\n",
    "    sigma = np.sum(fft_norm[cond_sigma])\n",
    "    # beta\n",
    "    cond_beta = np.where((freq>=FREQ_BANDS['beta'][0]) &\n",
    "                          (freq<=FREQ_BANDS['beta'][1]))\n",
    "    beta = np.sum(fft_norm[cond_beta])\n",
    "    # gamma\n",
    "    cond_gamma = np.where((freq>=FREQ_BANDS['gamma'][0]) &\n",
    "                          (freq<=FREQ_BANDS['gamma'][1]))\n",
    "    gamma = np.sum(fft_norm[cond_gamma])\n",
    "    \n",
    "    # R\n",
    "    R = (alpha + sigma) / beta\n",
    "    \n",
    "    return np.array([delta, theta, alpha, sigma, beta, gamma, R])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_processing(input_directory, destination, eval_mode = None, participant = None):\n",
    "    id = 0\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "\n",
    "        if filename.find('DS_Store') != -1: continue\n",
    "            \n",
    "        raw_eeg = np.load(input_directory+filename)\n",
    "\n",
    "        if filename.find('low') != -1:\n",
    "            gt = np.array([1])\n",
    "        elif filename.find('medium') != -1:\n",
    "            gt = np.array([5])\n",
    "        elif filename.find('high') != -1:\n",
    "            gt = np.array([10])\n",
    "        elif eval_mode == True:\n",
    "            gt = np.array([0])\n",
    "            name = filename[filename.find(participant):]\n",
    "\n",
    "        m = raw_eeg.shape[0]\n",
    "        n = raw_eeg.shape[1]\n",
    "        raw_eeg = raw_eeg.reshape((n,m))\n",
    "                \n",
    "        filtered_eeg = bandpass_filter(raw_eeg)\n",
    "\n",
    "        #Grabbing the 7 PSD values of each electrode\n",
    "        psd_one = psd_c1(filtered_eeg[0])\n",
    "        psd_two = psd_c1(filtered_eeg[1])\n",
    "        psd_three = psd_c1(filtered_eeg[2])\n",
    "        psd_four = psd_c1(filtered_eeg[3])\n",
    "        psd_five = psd_c1(filtered_eeg[4])\n",
    "        \n",
    "        min_f = np.min(filtered_eeg, axis=1)\n",
    "        max_f = np.max(filtered_eeg, axis=1)\n",
    "        avg_f = np.mean(filtered_eeg, axis=1)\n",
    "        var_f = np.var(filtered_eeg, axis=1)\n",
    "        med_f = np.median(filtered_eeg, axis=1)\n",
    "\n",
    "        #Should return shape of 1 X 61 (1+5X5+5X7)\n",
    "\n",
    "        features = np.concatenate([gt,min_f, max_f, avg_f, var_f, med_f, psd_one, psd_two, psd_three, psd_four,psd_five]).reshape(-1,61)\n",
    "\n",
    "        if eval_mode == True:\n",
    "            np.save(destination+str(name)+\".npy\", features.squeeze())\n",
    "        else:\n",
    "            np.save(destination +str(id)+\".npy\", features.squeeze())\n",
    "        id += 1\n",
    "\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_feature_extraction(input_directory, destination):\n",
    "    \n",
    "    high = []\n",
    "    low = []\n",
    "    for filename in os.listdir(input_directory):\n",
    "\n",
    "        if filename.find('DS_Store') != -1: continue\n",
    "        if filename.find('.sav') != -1: continue\n",
    "            \n",
    "        features = np.load(input_directory+filename, allow_pickle = True)\n",
    "        \n",
    "        if features[0] == 10:\n",
    "            high.append(features)\n",
    "        else:\n",
    "            low.append(features)\n",
    "        \n",
    "    high = np.array(high)\n",
    "    low = np.array(low)\n",
    "    \n",
    "    id = 0\n",
    "    \n",
    "    result_features = []\n",
    "    feature_difference = []\n",
    "    \n",
    "    #print(\"High\", high)\n",
    "    #print(\"low\", low)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx,example_high in enumerate(high):\n",
    "        \n",
    "        for idj, example_low in enumerate(low):\n",
    "        \n",
    "            if id % 2 == 0:\n",
    "                feature_difference = np.concatenate([[1], example_high[1:] - example_low[1:]])\n",
    "            else:\n",
    "                feature_difference = np.concatenate([[0], example_low[1:] - example_high[1:]])\n",
    "            np.save(destination +str(id)+\".npy\", feature_difference.squeeze())\n",
    "            \n",
    "            \n",
    "            id += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''raw_data_path = \"./data/eric/tmp/\"\n",
    "clean_data_path = \"./data/eric/clean/\"\n",
    "features_data_path = \"./data/eric/features/\"'''\n",
    "\n",
    "raw_data_path = \"./data/train/tmp/\"\n",
    "clean_data_path = \"./data/train/clean/\"\n",
    "features_data_path = \"./data/train/features/\"\n",
    "comparison_data_path = \"./data/train/comparison_features/\"\n",
    "\n",
    "hz = 250\n",
    "time_to_cut = 0\n",
    "NUM_ELECTRODES = 5\n",
    "\n",
    "#The features we shall extract for our linear regression model\n",
    "\n",
    "'''FREQ_BANDS = {\"slow\": [0.7, 2.0],\n",
    "              \"delta\": [1, 3.99],\n",
    "              \"theta0\": [4, 5.99],\n",
    "              \"theta1\": [6, 7.99],\n",
    "              \"theta2\": [4, 8.5],\n",
    "              \"alpha0\": [8, 9.99],\n",
    "              \"alpha1\": [10, 12.99],\n",
    "              \"beta0\": [13, 21.49],\n",
    "              \"beta1\" : [21.5, 29.99],\n",
    "              \"vertex\": [3, 5],\n",
    "              \"spindle\": [12, 14]}'''\n",
    "\n",
    "FREQ_BANDS = {\"delta\": [0.5 , 4.5],\n",
    "              \"theta\": [4.5 , 8.5],\n",
    "              \"alpha\": [8.5 , 11.5],\n",
    "              \"sigma\": [11.5, 15.5],\n",
    "              \"beta\" : [15.5, 30],\n",
    "              \"gamma\": [30  , 100] }\n",
    "FREQ_BANDS = OrderedDict(FREQ_BANDS)\n",
    "\n",
    "\n",
    "data_slicing(raw_data_path, clean_data_path)\n",
    "feature_processing(clean_data_path, features_data_path)\n",
    "comparison_feature_extraction(features_data_path, comparison_data_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, copy\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDatasetFunction(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, mode, max_path=None, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        #self.max_features = np.load(max_path)\n",
    "        \n",
    "        for filename in os.listdir(data_path):\n",
    "            if filename.find('.DS') != -1: continue\n",
    "            index = int(filename[:-4])\n",
    "            if index % 10 == 0:\n",
    "                test.append(np.load(data_path+filename, allow_pickle = True))\n",
    "            else:\n",
    "                train.append(np.load(data_path+filename, allow_pickle = True))\n",
    "                \n",
    "        self.data = train if mode == \"train\" else test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        #print(\"HELLO\")\n",
    "        #print(\"This is shape\", np.array(self.data).shape)\n",
    "        data = self.data[idx][1:].astype(np.float)\n",
    "        #data = data / self.max_features\n",
    "        #why are we dividing by the max?\n",
    "        label = self.data[idx][0]\n",
    "        #stored as the label\n",
    "        \n",
    "        if self.transform != None:\n",
    "            data = self.transform(label)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 629, 'test': 70}\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "train_set = FeatureDatasetFunction(comparison_data_path,\"train\")\n",
    "train_loader = DataLoader(train_set, batch_size=16, num_workers=4, shuffle=True)\n",
    "\n",
    "test_set = FeatureDatasetFunction(comparison_data_path,\"test\")\n",
    "test_loader = DataLoader(test_set, batch_size=16, num_workers=4)\n",
    "\n",
    "dataloaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": len(train_set), \"test\": len(test_set)}\n",
    "\n",
    "print(dataset_sizes)\n",
    "print(len(train_set.__getitem__(100)[0]))\n",
    "\n",
    "#60 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            model.train()\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                #print(\"prev input.shape{}\".format(inputs.shape))\n",
    "                inputs = inputs.float().squeeze()\n",
    "                #print(\"after input.shape{}\".format(inputs.shape))\n",
    "                #print(\"prev labels.shape{}\".format(labels.shape))\n",
    "                labels = labels.float().squeeze()\n",
    "                #print(\"after labels.shape{}\".format(labels.shape))\n",
    "\n",
    "                # forward\n",
    "                # Only calculate gradient for training\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs).squeeze()  \n",
    "                    #calcuting the loss based on the BCE Criterion\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = torch.round(outputs)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                \n",
    "            if phase == 'train' and scheduler != None:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=60, out_features=30, bias=False)\n",
      "  (1): Linear(in_features=30, out_features=10, bias=False)\n",
      "  (2): Linear(in_features=10, out_features=1, bias=False)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_model = nn.Sequential(nn.Linear(60,30,bias=False),nn.Linear(30,10,bias=False), nn.Linear(10,1,bias=False), nn.Sigmoid())\n",
    "criterion = nn.BCELoss()\n",
    "print(my_model)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.SGD(my_model.parameters(), lr=0.5)\n",
    "optimizer_ft = optim.Adam(my_model.parameters(),lr=0.05, weight_decay=0.0001)\n",
    "lr_scheduler = None\n",
    "# lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.FeatureDatasetFunction'>: it's not the same object as __main__.FeatureDatasetFunction",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e5827c36d29b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m final_model = train_model(my_model, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                        \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                        \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                        \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                        \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-4f71d3ed64ba>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mset_spawning_popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mForkingPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.FeatureDatasetFunction'>: it's not the same object as __main__.FeatureDatasetFunction"
     ]
    }
   ],
   "source": [
    "final_model = train_model(my_model, \n",
    "                                                       criterion, \n",
    "                                                       optimizer_ft, \n",
    "                                                       lr_scheduler, \n",
    "                                                       dataloaders, \n",
    "                                                       dataset_sizes,\n",
    "                                                       num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the best features to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figuring out how many features to include\n",
    "import pickle\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Returns the ideal number of features to include in the linear regression model\n",
    "def select_best_features(X_train, y_train, X_test):\n",
    "    results = []\n",
    "    num_total_features = X_train.shape[1]\n",
    "    \n",
    "    for var in range(num_total_features + 1)[1:]:\n",
    "        lin_reg_mod = LinearRegression()\n",
    "\n",
    "        #Trying out different number of features to include\n",
    "        X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test, var)\n",
    "        lin_reg_mod.fit(X_train_fs, y_train)\n",
    "        y_pred= lin_reg_mod.predict(X_test_fs)\n",
    "\n",
    "        test_set_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        test_set_r2 = r2_score(y_test, y_pred)\n",
    "        results.append([test_set_r2, var, test_set_rmse])\n",
    "        \n",
    "    results = sorted(results)\n",
    "    #print(results)\n",
    "        \n",
    "    return results[-1][1], results\n",
    "\n",
    "#Returns the X_train_fs, X_test_fs, and the feature selection model itself\n",
    "def select_features(X_train, y_train, X_test, num_features):\n",
    "    \n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=f_regression, k = num_features)\n",
    "    \n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.fit_transform(X_train, y_train)\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def pca_data(X, X_test, N):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_test = StandardScaler().fit_transform(X_test)\n",
    "    pca = PCA(n_components = N)\n",
    "    X = pca.fit_transform(X)\n",
    "    X_test = pca.fit_transform(X_test)\n",
    "    print(\"pca_components * pca_explained_variance\", pca.explained_variance_ratio_)\n",
    "    \n",
    "    print(\"Total explained variance, \", np.sum(pca.explained_variance_ratio_))\n",
    "    return X, X_test, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (24, 60) (24,)\n",
      "Test (6, 60) (6,)\n",
      "pca_components * pca_explained_variance [0.47234303 0.23191044]\n",
      "Total explained variance,  0.7042534668003213\n",
      "(24, 2)\n"
     ]
    }
   ],
   "source": [
    "#Gotta save the pca and fs objects locally\n",
    "\n",
    "X_train, X_test, y_train, y_test = training_data(features_data_path)\n",
    "\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# feature selection for linear regression model\n",
    "final_num_features, results = select_best_features(X_train, y_train, X_test)\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test, final_num_features)\n",
    "\n",
    "X_train_pca, X_test_pca, pca_object = pca_data(X_train, X_test, 2)\n",
    "print(X_train_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of features to have is 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42 49 56 51 33 18]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Best number of features to have is\",final_num_features)\n",
    "#print(X_train_fs.shape, X_test_fs.shape)\n",
    "#what are scores for the features\n",
    "#for i in range(len(fs.scores_)):\n",
    "    #print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()\n",
    "\n",
    "#print(results)\n",
    "features_included = fs.scores_.argsort()[-final_num_features:][::-1]\n",
    "print(features_included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [3.42074025 6.66569964 6.06288736 4.50125965 7.95202902 2.59166567] Real:  [ 1. 10. 10.  1. 10.  1.]\n",
      "Final R2 Score: 0.5764213819595321 \n",
      " FInal RMSE Score: 2.9287312979034925\n"
     ]
    }
   ],
   "source": [
    "lin_reg_mod = LinearRegression()\n",
    "\n",
    "lin_reg_mod.fit(X_train_fs, y_train)\n",
    "y_pred= lin_reg_mod.predict(X_test_fs)\n",
    "\n",
    "#lin_reg_mod.fit(X_train_pca, y_train)\n",
    "#y_pred= lin_reg_mod.predict(X_test_pca)\n",
    "\n",
    "print(\"Prediction: \", y_pred, \"Real: \", y_test)\n",
    "\n",
    "test_set_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "test_set_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Final R2 Score:\", test_set_r2, \"\\n FInal RMSE Score:\", test_set_rmse)\n",
    "\n",
    "model_name = 'finalmodel.sav'\n",
    "pickle.dump(lin_reg_mod, open(features_data_path+model_name,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#y_pred = loaded_model.predict(X_test_fs)\n",
    "#print(y_pred)\n",
    "\n",
    "#test_set_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "#test_set_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#print(test_set_r2, test_set_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_processing(input_directory, fs, eval_mode = None, pca = None):\n",
    "    x_final = []\n",
    "    y_final = []\n",
    "    \n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.find('.DS_Store') != -1: continue\n",
    "            \n",
    "        data = np.load(input_directory+filename)\n",
    "        #print(data.shape)\n",
    "        x = data[1:]\n",
    "        x_final.append(x.reshape(-1, 1))\n",
    "        \n",
    "        if eval_mode == True:\n",
    "            y = filename\n",
    "            y_final.append(y)\n",
    "        else:\n",
    "            y = data[0]\n",
    "            y_final.append(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    x_final = np.array(x_final).squeeze()\n",
    "    \n",
    "    if eval_mode == True:\n",
    "        y_final = y_final\n",
    "    else:\n",
    "        y_final = np.array(y_final).squeeze()\n",
    "    \n",
    "    if pca != None:\n",
    "        x_final = pca_object.transform(x_final)\n",
    "    else:\n",
    "        x_final = fs.transform(x_final)\n",
    "    \n",
    "    return x_final, y_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 6)\n",
      "12 16\n",
      "-0.29198337762097193\n",
      "Final comparison accuracy 0.75\n"
     ]
    }
   ],
   "source": [
    "test_raw_data_path = \"./data/eric/test/tmp/\"\n",
    "test_clean_data_path = \"./data/eric/test/clean/\"\n",
    "test_features_data_path = \"./data/eric/test/features/\"\n",
    "model_path = features_data_path + model_name\n",
    "\n",
    "\n",
    "def accuracy_evaluation(y_final, y_pred):\n",
    "    #Figuring out the total comparisions I need to make\n",
    "    \n",
    "    #length = float(len(y_final))\n",
    "    #total_comparisons = 0.5*(length**2 + length)\n",
    "    comparisons = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for idx,val in enumerate(y_final):\n",
    "        \n",
    "        if idx == (len(y_final)) - 1: break\n",
    "        idx2 = idx+1\n",
    "        \n",
    "        for idx2 in range(len(y_final)):\n",
    "            if y_final[idx] > y_final [idx2]:\n",
    "                total += 1 \n",
    "                comparisons.append([idx,idx2])\n",
    "                \n",
    "    for pair in comparisons:\n",
    "        if y_pred[pair[0]] > y_pred[pair[1]]:\n",
    "            correct += 1\n",
    "    print(correct, total)\n",
    "    return float(correct)/total\n",
    "\n",
    "\n",
    "def evaluate_model(test_raw_data_path, test_clean_data_path, test_features_data_path, model_path, eval_mode = None, participant = None, pca = None):\n",
    "    \n",
    "    data_slicing(test_raw_data_path, test_clean_data_path, eval_mode, participant)\n",
    "    feature_processing(test_clean_data_path, test_features_data_path, eval_mode, participant) #filter and PSD data\n",
    "    if pca != None:\n",
    "        X_final,y_final = testing_data_processing(test_features_data_path, fs, eval_mode = eval_mode, pca = pca)\n",
    "    else:\n",
    "        X_final, y_final = testing_data_processing(test_features_data_path, fs, eval_mode = eval_mode)\n",
    "\n",
    "    loaded_model = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "    print(X_final.shape)\n",
    "    y_pred = loaded_model.predict(X_final)\n",
    "    \n",
    "    #print(\"y_final\",y_final)\n",
    "    #print(y_pred)\n",
    "    \n",
    "    if eval_mode == True:\n",
    "        predictions = []\n",
    "        for val in range(len(y_pred)):\n",
    "            predictions.append([y_pred[val], y_final[val]])\n",
    "        predictions = sorted(predictions)[::-1]\n",
    "        \n",
    "        for elem in predictions:\n",
    "            print(elem)\n",
    "        return\n",
    "    accuracy = accuracy_evaluation(y_final, y_pred)\n",
    "\n",
    "    test_set_r2 = r2_score(y_final, y_pred)\n",
    "    print(test_set_r2)\n",
    "    print(\"Final comparison accuracy\", accuracy)\n",
    "    \n",
    "evaluate_model(test_raw_data_path, test_clean_data_path, test_features_data_path, model_path)\n",
    "#evaluate_model(test_raw_data_path, test_clean_data_path, test_features_data_path, model_path, pca = pca_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained this linear regression model with eric's EEG when he looks at pictures of pretty girls and his score for each girl. Our comparison method, however, involves comparing two the real scores of the pictures and the predicted score of the pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去测试了一下我的朋友george的脑电波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 6)\n",
      "[13.435484841966842, 'George_aurora.txt26.npy.npy']\n",
      "[13.43548484196684, 'George_aurora.txt19.npy.npy']\n",
      "[12.928310561973461, 'George_courtney.txt11.npy.npy']\n",
      "[8.84358634965543, 'George_DiLiReBa.txt2.npy.npy']\n",
      "[8.414512775944068, 'George_20210529173742_8s_BrainUp70B4AE2CBDA.txt20.npy.npy']\n",
      "[7.952029018208597, 'George_liu.txt6.npy.npy']\n",
      "[7.492056982267195, 'George_jennifer.txt1.npy.npy']\n",
      "[7.016152900019309, 'George_sherry.txt25.npy.npy']\n",
      "[7.016152900019309, 'George_sherry.txt18.npy.npy']\n",
      "[6.078769851308899, 'George_20210529173713_8s_BrainUp70B4AE2CBDA.txt22.npy.npy']\n",
      "[5.904774734942273, 'George_marg.txt27.npy.npy']\n",
      "[5.904774734942273, 'George_marg.txt20.npy.npy']\n",
      "[5.609528883003723, 'George_Diana.txt28.npy.npy']\n",
      "[5.609528883003723, 'George_Diana.txt21.npy.npy']\n",
      "[5.24926302186629, 'George_stela.txt30.npy.npy']\n",
      "[5.24926302186629, 'George_stela.txt23.npy.npy']\n",
      "[4.897987488559653, 'George_yangzi.txt7.npy.npy']\n",
      "[4.501259646413079, 'George_20210529173728_8s_BrainUp70B4AE2CBDA.txt24.npy.npy']\n",
      "[4.383547287832454, 'George_20210529173657_8s_BrainUp70B4AE2CBDA.txt19.npy.npy']\n",
      "[4.225762878561914, 'George_20210529173622_8s_BrainUp70B4AE2CBDA.txt21.npy.npy']\n",
      "[-0.06808646008357755, 'George_20210529173643_8s_BrainUp70B4AE2CBDA.txt15.npy.npy']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "george_raw_data_path = \"./data/george/tmp/\"\n",
    "george_clean_data_path = \"./data/george/clean/\"\n",
    "george_features_data_path = \"./data/george/features/\"\n",
    "model_path = features_data_path + model_name\n",
    "\n",
    "evaluate_model(george_raw_data_path, george_clean_data_path, george_features_data_path, model_path, eval_mode = True, participant = \"George\")\n",
    "#evaluate_model(george_raw_data_path, george_clean_data_path, george_features_data_path, model_path, eval_mode = True, participant = \"George\", pca = pca_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainup_env",
   "language": "python",
   "name": "brainup_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
